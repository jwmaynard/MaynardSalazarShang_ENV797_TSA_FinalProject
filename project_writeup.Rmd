---
title: "project_writeup"
author: "Pablo Salazar, Ellie Shang, Justin Maynard"
date: "2025-04-25"
output: pdf_document
---

```{r include=FALSE}
#Import libraries
library(agridat)
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(cowplot)
library(smooth)
library(kableExtra)
library(readxl)
theme_set(theme_classic())
#tinytex::reinstall_tinytex(repository = "illinois")

```


### Purpose


### Soybean
Soybean data consists of yearly yields from 1975 to 2023, from the USDA's Commodity Cost and Returns report. According to the USDA, 70% of soybeans grown in the US are used for animal feed, with 15% used for human consumption, and 5% used for biodiesel. Globally, Brazil accounts for 40% of soybean production, while the United States accounts for 28%. In the United States soybeans are planted in May or June, and harvested in September or October. Soybeans are grown in the Midwest and allong the Mississippi River, with Illinois accounting for 16% of production, Iowa accounting for 14%, and both Minnesota and Ohio accounting for 8%.
\
#### Data exploration

```{r message=FALSE, warning=FALSE, include=FALSE}
soybean <- read_excel("data/HUSSoyb.xls", sheet = 'cleaned')
colnames(soybean) <- c("Year", "yield")

soybean <- soybean %>% 
  group_by(Year) %>% 
  summarise(yield = mean(yield))



nfor_soy <- round(nrow(soybean) * .10)
nobs_soy <- nrow(soybean)
nvar <- ncol(soybean)

```


```{r fig.height=3, fig.width=5, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(soybean) + 
  geom_line(aes(x = Year, y = yield)) + 
  xlab("Year") +
  ylab("Yield") +
  theme_classic()
```


```{r warning=FALSE, message=FALSE,include=FALSE}
ts_soybean_full <- ts(soybean$yield,
                      start = soybean$Year[1],
                      frequency = 1)

ts_soybean_test <- ts(soybean$yield[(nobs_soy-nfor_soy+1):nobs_soy],
                      start = soybean$Year[nobs_soy-nfor_soy+1],
                      frequency = 1)

ts_soybean_train <- ts(soybean$yield[1:(nobs_soy-nfor_soy)],
                 start = soybean$Year[1],
                 frequency = 1)

adf <- (adf.test(ts_soybean_full,alternative="stationary"))
pvalue <- round(adf$p.value,2)
#P value greater than .05 so we fail to reject H0, data has stochastic trend
```

When examining soybean yield over time, it has appeared increase linearly with wide variations. After conducting an Augmented Dickey Fuller test, we find that the p value is `r pvalue`. Since this is above 0.05, we fail to reject H0, and our soybean yield data has stochastic trend, meaning it can change randomly.

```{r, warning=FALSE, message=FALSE,include=FALSE}


ts <- autoplot(ts_soybean_full, plot = FALSE, ylab = "Yield (bu./acre)")

pacf <- autoplot(Pacf(ts_soybean_full, lag = 40, plot=FALSE),  
                  main = "PACF of Yield")

acf <- autoplot(Acf(ts_soybean_full, lag = 40, plot=FALSE),  
                  main = "ACF of Yield")


```

```{rr fig.height=3, fig.width=5, message=FALSE, warning=FALSE, echo=FALSE}
plot_grid(ts, pacf, acf,
          nrow = 3)

```
When examining the PACF and ACF of our data we observe a cutoff in the PACF and a gradual decline in the ACF, meaning our data exhibits autoregressive characteristics. An autoregressive component and stochastic trend means that yield depends on previous observations and a stochastic term.


#### Models

\
Next, we trained various models on our time series, using a 10% holdout (five years) to test our models. We began by testing simple methods including the simple moving average and simple exponential smoothing. Next, we used an auto arima model, which determined an Arima(3,1,0) as the optimal model. This was interesting as it looked as if our cutoff was at lag one, not lag three. A TBATS, neural network, state space exponential smoothing, and structural time series were also used. The neural network's p and P values were chosen by experimenting to see what yielded the closest fit. The state space exponential smoothing was an ETS(AAN) local trend model.

```{r warning=FALSE, message=FALSE,include=FALSE}

#simple moving average
sma_soy <- sma(y = ts_soybean_train, h = nfor_soy, holdout = FALSE, silent = FALSE)

#simple exponential smoothing
ses_soy <- ses(y = ts_soybean_train, h = nfor_soy, holdout = FALSE, silent = FALSE)

#auto sarima
sarima_soy <- auto.arima(ts_soybean_train)
sarima_forecast <- forecast(object = sarima_soy, h = nfor_soy)

checkresiduals(sarima_forecast)
sarima_residuals <- autoplot(sarima_forecast$residuals, ylab = "", main = "Residuals")


sarima_scores <- accuracy(sarima_forecast$mean, ts_soybean_test)
sma_scores <- accuracy(sma_soy$forecast, ts_soybean_test)
ses_scores <- accuracy(ses_soy$mean, ts_soybean_test)



```

```{r warning=FALSE, message=FALSE,include=FALSE}
TBATS_fit <- tbats(ts_soybean_train)
TBATS_forecast <- forecast(TBATS_fit, h = nfor_soy)

TBATS_scores <- accuracy(TBATS_forecast$mean, ts_soybean_test)

```

```{r warning=FALSE, message=FALSE,include=FALSE}

NN_fit <- nnetar(ts_soybean_train,
                 P = 0,
                 p = 5)
                
Nnfor_soyecast <- forecast(NN_fit,
                        h = nfor_soy) 

NN_scores <- accuracy(Nnfor_soyecast$mean, ts_soybean_test)

```

```{r warning=FALSE, message=FALSE,include=FALSE}
SSES_fit <- es(ts_soybean_train, model = "ZZZ", h = nfor_soy, holdout = FALSE)
SSES_forecast <- forecast(SSES_fit, h = nfor_soy)

checkresiduals(SSES_fit)

SSES_scores <- accuracy(SSES_forecast, ts_soybean_test)



StructTS_fit <- StructTS(ts_soybean_train,
                   type = "trend")
StructTS_forecast <- forecast(StructTS_fit, h = nfor_soy)

checkresiduals(StructTS_fit)

StructTS_scores <- accuracy(StructTS_forecast$mean, ts_soybean_test)

```



#### Scenario generation

\
To explore effects of different variables on soybean yield we next used scenario generation. Data from NOAA's "Climate at a Glance" state and regional time series was used. This provided data such as precipitation, Palmer Drought Severity Index (PDSI), Palmer Hydrological Drought Index (PHDI), and Palmer Modified Drought Index (PMDI). We downloaded precipitation and PDSI data from key soybean production states including Illinois, Indiana, Iowa, and Minnesota. We also downloaded country wide precipitation, PDHI, PDSI, and PMDI data. Lastly, we downloaded precipitation and PDSI data from the soybean belt (both area weighted and productivity weighted). This data was aggregated to the year and combined with our soybean yield. Time series, following the same ten percent holdout rule, were created. Correlation between variables was then calculated, and forecast generation was conducted using auto arima and 1000 scenarios. 


```{r include=FALSE, warning=FALSE, message=FALSE,include=FALSE}
data_folder <- "/home/guest/MaynardSalazarShang_ENV797_TSA_FinalProject/data/"

files <- list.files(data_folder, pattern = "i.csv")

for(i in files){
  df_name <- sub("\\.csv$","",basename(i))
  tmp <- read_csv(paste0(data_folder, i), skip = 2) %>% 
                  mutate(Date = ymd(paste0(Date, "01")),
                         Year = year(Date)) %>% 
    group_by(Year) %>% 
    summarise(
      across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
      .groups = "drop"
    )
  
  assign(paste0(df_name, "_annual"), tmp, envir = .GlobalEnv)
}

files <- list.files(data_folder, pattern = "precipitation.csv")

for(i in files){
  df_name <- sub("\\.csv$","",basename(i))
  tmp <- read_csv(paste0(data_folder, i), skip = 3) %>% 
                  mutate(Date = ymd(paste0(Date, "01")),
                         Year = year(Date)) %>% 
    group_by(Year) %>% 
    summarise(
      across(where(is.numeric), ~ mean(.x, na.rm = TRUE)),
      .groups = "drop"
    )
  
  assign(paste0(df_name, "_annual"), tmp, envir = .GlobalEnv)
}

```


```{r warning=FALSE, message=FALSE,include=FALSE}

#Grab all *_annual data-frames that exist in the workspace
annual_names <- ls(pattern = "_annual$")  # character vector
annual_list  <- mget(annual_names) # named list of data-frames

#Rename each Value column to the data-frame’s name
annual_list <- imap(annual_list, ~
  rename(.x, !!.y := Value) # .y is the name
)

#Reduce the list with full_join() and add soybean
state_values <- reduce(annual_list, inner_join, by = "Year")

scenario_df <- soybean %>% #assumes cols Year, yield
  rename(yield = yield) %>%  #change if needed
  full_join(state_values, by = "Year") %>% #combine everything
  arrange(Year) #tidy ordering

```


```{r warning=FALSE, message=FALSE,include=FALSE}
#cor(scenario_df[,2:ncol(scenario_df)])
ncol_soy <- ncol(scenario_df)


scenario_ts <- ts((scenario_df[,2:ncol_soy]), start = c(1975,1), frequency = 1)

scenario_ts_train <- ts((scenario_df[1:(nobs_soy-nfor_soy),2:ncol_soy]), start = c(1975,1), frequency = 1)

scenario_ts_test <- ts((scenario_df[(nobs_soy-nfor_soy+1):nobs_soy,2:ncol_soy]), start = scenario_df$Year[nobs_soy-nfor_soy+1], frequency = 1)


R = cor(scenario_ts_train)


horizon=nfor_soy  #we want to forecast two years ahead in monthly steps
nscen=1000    #number of scenarios to be generated 

X=array(0,c(ncol(scenario_ts_train),horizon,nscen)) #array where we will store the independently generated scenarios 


```

```{r warning=FALSE, message=FALSE,include=FALSE}

# Need to do a loop over all variables under analysis or repeat process 3 times
for(i in 1:ncol(scenario_ts_train)){  
  
  # Fit a SARIMA model
  # Note I am fixing a few parameters regarding the order of the model 
  # just to help auto.arima() converge faster
  
  fit_SARIMA=auto.arima(scenario_ts_train[,i]) #,max.d=1,max.D=1,max.p=1,max.P=1,max.Q=1) 
  
  for_SARIMA=forecast(fit_SARIMA, h=horizon)   #forecast using the fitted SARIMA
  
  #Generating scenarios
  # to generate scenarios we will need standard deviation of residuals
  # forecast() function does not directly output the standard error we will need to calculate it

  for(t in 1:horizon){
    # we will use the following expression to manually compute sd
    sd=(for_SARIMA$upper[t,1] - for_SARIMA$lower[t,1]) / (2 * qnorm(.5 + for_SARIMA$level[1] / 200))
    
    # Now that I have mean and standard deviation for time t
    # I can draw scenarios using the rnorm() function
    X[i,t,]=rnorm(nscen,mean=for_SARIMA$mean[t],sd=sd)  
    
    #note this is done in a loop for all the 24 steps we are forecasting 
    #and this loop is inside a loop over all HPP inflows
    
  } # end t loop

  # remove models just to make sure we start from scratch for the next HPP
  # remember we are still inside the HPP loop
  rm(fit_SARIMA, for_SARIMA) 
                            
}#end HPP loop

```

```{r warning=FALSE, message=FALSE,include=FALSE}

U <- chol(R) #that will give upper triangular matrix for Cholesky decomposition
L <- t(U) #to get lower triangular matrix you need to transpose U, that is what the t() function is doing here

#Creating array Y where we will store correlated scenarios
Y <- array(0,c(ncol(scenario_ts),horizon,nscen)) 

# Need to use another loop structure to make sure spatial correlation among HPP is present in all scenarios
for(s in 1:nscen){ 
  aux <- X[,,s] #creating aux variable simple because X is not a 2x2 matrix, 
                  #but an array of 3 dimension and we cannot do matrix multiplication with arrays
  
  Y[,,s] <- L%*%aux  #recall L is the Cholesky decomposition of our correlation matrix R computed from with historical data

}#end scenario loop
```


```{r warning=FALSE, message=FALSE,include=FALSE}

# 1. build a “long” tibble of all scenarios ---------------------------

# the vector of years in your test period:
yield <- 1
test_years <- soybean$Year[(nobs_soy-nfor_soy+1):nobs_soy]

# assemble
scenario_df <- 
  expand.grid(
    Year     = test_years,
    scenario = seq_len(nscen)
  ) %>% 
  arrange(Year, scenario) %>% 
  mutate(
    value = as.vector(t(Y[yield, , ]))
  )

# 2. compute summary bands (e.g. median, 10th/90th pct) ------------------

fan_df <- scenario_df %>%
  group_by(Year) %>%
  summarise(
    p50   = median(value),
    p10   = quantile(value, .10),
    p90   = quantile(value, .90),
    .groups = "drop"
  )

# 3. pull out the actual test data --------------------------------------

actual_df <- 
  tibble(
    Year  = test_years,
    actual = as.numeric(ts_soybean_test)
  )

# 4. spaghetti + fan + actual overlay -----------------------------------
scenario_plot <- ggplot() +
    # a) light gray spaghetti
    geom_line(data = scenario_df,
              aes(Year, value, group = scenario),
              color = "gray60", alpha = 0.3) +
    # b) fan‐chart ribbon
    geom_ribbon(data = fan_df,
                aes(Year, ymin = p10, ymax = p90),
                fill = "steelblue", alpha = 0.25) +
    # c) median line
    geom_line(data = fan_df,
              aes(Year, p50),
              color = "steelblue", size = 1) +
    # d) actual held‐out
    geom_line(data = actual_df,
              aes(Year, actual),
              color = "firebrick", size = 1) +
    geom_point(data = actual_df,
               aes(Year, actual),
               color = "firebrick", size = 2) +
    labs(title    = "Soybean Yield: scenarios vs actual",
         y        = "Yield",
         subtitle = "Gray = each scenario; blue band = 10–90% range; red = actual") +
    theme_classic()
```
After visualizing our scenarios, we can see that our 10% to 90% confidence band captures all actual scenarios, with the mean of our scenarios closely following the actual data.
\
```{r fig.height=3, fig.width=5, message=FALSE, warning=FALSE, echo=FALSE}
scenario_plot
```


```{r warning=FALSE, message=FALSE,include=FALSE }

fc <- list(
  model  = "Simulated",
  level  = c(10, 90),
  mean   = ts(fan_df$p50,
              start = start(ts_soybean_test),
              frequency = frequency(ts_soybean_test)),
  lower  = ts(cbind(fan_df$p10, fan_df$p10),  # shape: time × n_levels
              start = start(ts_soybean_test),
              frequency = frequency(ts_soybean_test)),
  upper  = ts(cbind(fan_df$p90, fan_df$p90),
              start = start(ts_soybean_test),
              frequency = frequency(ts_soybean_test)),
  x      = ts_soybean_train,
  series = "Soybean yield",
  method = "Cholesky‐sim"
)

autoplot(fc$mean) +
  autolayer(ts_soybean_test, series = "Actual") +
  labs(title = "Simulated soybean forecasts vs actuals")

scenario_scores <- accuracy(fc$mean, ts_soybean_test)
```



#### Accuracy and recommendations


```{r warning=FALSE, message=FALSE,include=FALSE}

forecasts_all_plot <- autoplot(ts_soybean_full) +
  autolayer(sma_soy$forecast, series = "Simple Moving Average") +
  autolayer(ses_soy$mean, series = "Simple Exponential Smoothing") +
  autolayer(sarima_forecast$mean, series = "ARIMA(3,1,0)") +
  autolayer(TBATS_forecast$mean, series = "TBATS") +
  autolayer(Nnfor_soyecast$mean, series = "NNAR(5,3)") +
  autolayer(SSES_forecast$mean, series = "ETS(AAN)") +
  autolayer(StructTS_forecast$mean, series = "Structual TS") +
  autolayer(fc$mean, series = "Scenarios Mean") +
  ylab("Yield (bu./acre)") 

forecasts_all_plot_2018_2023 <- autoplot(ts_soybean_full) +
  autolayer(sma_soy$forecast, series = "Simple Moving Average") +
  autolayer(ses_soy$mean, series = "Simple Exponential Smoothing") +
  autolayer(sarima_forecast$mean, series = "ARIMA(3,1,0)") +
  autolayer(TBATS_forecast$mean, series = "TBATS") +
  autolayer(Nnfor_soyecast$mean, series = "NNAR(5,3)") +
  autolayer(SSES_forecast$mean, series = "ETS(AAN)") +
  autolayer(StructTS_forecast$mean, series = "Structual TS") +
  autolayer(fc$mean, series = "Scenarios Mean") +
  ylab("Yield (bu./acre)") +
  xlim(c(2018,2023))
  
```
\

```{r fig.height=3, fig.width=5, message=FALSE, warning=FALSE, echo=FALSE}
forecasts_all_plot_2018_2023
```



```{r warning=FALSE, message=FALSE,include=FALSE}
#edit to include all
#create data frame
soy_scores <- as.data.frame(rbind(NN_scores, sarima_scores, StructTS_scores, TBATS_scores, SSES_scores, ses_scores, sma_scores, scenario_scores))
row.names(soy_scores) <- c("NNAR(5,3)", "ARIMA(3,1,0)","Structural TS", "TBATS", "ETS(AAN)", "SES", "SMA", "Scenarios Mean")

#choose model with lowest RMSE
best_model_index <- which.min(soy_scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(soy_scores[best_model_index,]))       

scores <- kbl(soy_scores, 
      caption = "Forecast Accuracy for Soybean Models",
      digits = array(5,ncol(soy_scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(soy_scores[,"RMSE"]))


```
\

```{r echo=FALSE}
scores
```
\
When looking at the error metrics for our various forecasting techniques the exponential smoothing  has the lowest mean absolute percentage error and mean percentage error. Scenario generation is effective in creating scenarios based on possible precipitations and drought indices. 

### Rye
The dataset used for forecasting rye yields in the United States comes from the U.S. Department of Agriculture and includes annual data from 1866 to 2024. It captures long-term trends in rye production, with yields reported in bushels per acre. This measure reflects the efficiency of production rather than total output. In 2024, the U.S. harvested 402,000 acres of rye, resulting in a total production of 14.8 million bushels. This rich historical dataset provides a solid foundation for modeling and forecasting future yield patterns.

#### Data exploration
```{r}
# Set the path to your desktop (Windows example)
library(here)
here()
file_path <- paste0(here(),"/data/","Wheat Data-All Years.xlsx")



# Read the Excel file
data_rye <- read_excel(file_path, sheet = "Table02", skip = 1)
data_wheat <- read_excel(file_path, sheet = "Table04", skip = 1)

```

```{r}
wheat_p <- data_wheat$`U.S. production (million bushels)`  # Replace with the exact column name if different

# Create a time series starting from 1866 with yearly data
start_year_w <- 1960
wheat_ts <- ts(wheat_p, start = start_year_w, frequency = 1)


# Plot the time series
plot(wheat_ts, main = "U.S. production (million bushels)", ylab = "U.S. production (million bushels)", xlab = "Year")
```

```{r}
yield_data <- data_rye$`Yield (bushels per acre)`  # Replace with the exact column name if different

# Create a time series starting from 1866 with yearly data
start_year <- 1866
rye_ts <- ts(yield_data, start = start_year, frequency = 1)

# View the time series
print(rye_ts)

# Optionally, plot the time series
plot(rye_ts, main = "Rye Yield (Bushels per Acre)", ylab = "Yield", xlab = "Year")
```


```{r}
rye_ACF <- Acf(rye_ts, lag=40, plot=FALSE)
rye_PACF <- Pacf(rye_ts, lag=40, plot=FALSE)

autoplot(rye_ACF) + theme_classic()
autoplot(rye_PACF) + theme_classic()
```


```{r}
rye_ts

```
```{r}
rye_tsc <- na.omit(rye_ts)
length(rye_tsc)
```

```{r}
nobs <- length(rye_tsc)
t <- c(1:nobs)

linear_trend_model_rye <- lm(rye_tsc ~ t) 
summary(linear_trend_model_rye)

#saving regression coefficients
beta0_rye <- as.numeric(linear_trend_model_rye$coefficients[1])
beta1_rye <- as.numeric(linear_trend_model_rye$coefficients[2])
```

>Across the full time series, rye yields in the U.S. increased at an average rate of 0.15 bushels per acre per year. However, this growth was not consistent over time. Between 1950 and 2000, the rate of increase significantly accelerated, with yields rising by an average of 0.32 bushels per acre per year—more than double the long-term trend.

#### Models
> To improve the accuracy and reliability of our time series forecasting for U.S. rye yields (measured in bushels per acre), we deliberately limited the dataset to include only data from 1920 onward. This decision was made to minimize potential skewness introduced by earlier, less consistent historical data, and to better reflect the agricultural practices, technologies, and climate conditions relevant to the modern era.


```{r}
# Original cleaned time series
start_year <- 1866
new_start <- 1920
new_end <- 2024

# Calculate the index position of the new year
start_index <- new_start - start_year + 1

# Create new time series from new year onward
rye_ts_ny <- window(rye_tsc, start = new_start, end = new_end)
```

```{r}
rye_ts_ny
```


```{r}
nobs <- length(rye_ts_ny)
t <- c(1:nobs)

linear_trend_model_rye_ny <- lm(rye_ts_ny ~ t) 
summary(linear_trend_model_rye_ny)

#saving regression coefficients
beta0_rye <- as.numeric(linear_trend_model_rye_ny$coefficients[1])
beta1_rye <- as.numeric(linear_trend_model_rye_ny$coefficients[2])
```



State space exponential smoothing
```{r}
#Plotting Data

ts <- autoplot(rye_ts, plot = FALSE) + theme_classic()

pacf <- autoplot(Pacf(rye_ts, lag = 40, plot=FALSE),  
                  main = "PACF") + theme_classic()

acf <- autoplot(Acf(rye_ts, lag = 40, plot=FALSE),  
                  main = "ACF") + theme_classic()

plot_grid(ts, pacf, acf,
          nrow = 3
)


```

```{r}
#Stationary Test

print((adf.test(rye_ts_ny,alternative="stationary")))
#P value less than .05 so we fail to reject H0, data has no stochastic trend
```
Forecasting Simple Techniques
```{r}
naive_rye <- naive(rye_ts_ny, h = 5)
plot(naive_rye)

```

```{r}
#simple moving average
sma_rye <- sma(y = rye_ts_ny, h = 5, holdout = FALSE, silent = FALSE)
plot(sma_rye)

autoplot(rye_ts_ny) + 
  autolayer(sma_rye$forecast)
```

```{r}
#simple exponential smoothing
ses_rye <- ses(y = rye_ts_ny, h = 5, holdout = FALSE, silent = FALSE)

plot(rye_ts_ny, col = 'black')
lines(ses_rye$fitted, col = 'blue')
```

```{r}
#auto arima
rye_ARIMA <- auto.arima(rye_ts_ny,seasonal = FALSE, lambda = 0)
rye_arima_forecast <-forecast(rye_ARIMA)

autoplot(rye_arima_forecast, ylab = "Yield (bushels per acre)")

arima_scores <- accuracy(rye_arima_forecast)

```



BATS

```{r}
TBATS_fit <- tbats(rye_ts_ny)
TBATS_forecast <- forecast(TBATS_fit)

autoplot(TBATS_forecast) + 
  autolayer((rye_ts_ny))

TBATS_scores <- accuracy(TBATS_forecast)

```

Neural Network

```{r}
NN_fit <- nnetar(rye_ts_ny,
                 P = 2,
                 p = 2)
                
NN_forecast <- forecast(NN_fit) 

autoplot(NN_forecast) + 
  autolayer(rye_ts_ny)

NN_scores <- accuracy(NN_forecast)

```

State space exponential smoothing
```{r}
SSES_rye <- es(rye_ts_ny, model = "ZZZ", h <- 10, holdout = FALSE)
plot(SSES_rye)
checkresiduals(SSES_rye)
SSES_rye$model

SS_fit <- StructTS(rye_ts_ny,
                   type = "trend")
checkresiduals(SS_fit)

SS_forecast <- forecast(SS_fit, h = 10)

autoplot(SS_forecast) +
  autolayer(rye_ts_ny)

SS_scores <- accuracy(SS_forecast)

```

#### Accuracy and recommendations
```{r}
# Combine all scores into one data frame
all_scores <- rbind(
  cbind(Model = "ARIMA", arima_scores),
  cbind(Model = "TBATS", TBATS_scores),
  cbind(Model = "NNETAR", NN_scores),
  cbind(Model = "StructTS", SS_scores)
)

# Convert to data frame for kable
all_scores <- as.data.frame(all_scores)

# Ensure numeric columns are correctly typed (in case they are characters from rbind)
numeric_cols <- setdiff(colnames(all_scores), "Model")
all_scores[numeric_cols] <- lapply(all_scores[numeric_cols], as.numeric)

# Find index of the row with the lowest RMSE
lowest_rmse_index <- which.min(all_scores[["RMSE"]])

# Create a styled table with the model that has the lowest RMSE highlighted
kbl(all_scores, 
    caption = "Forecast Accuracy Across Models",
    digits = array(5, ncol(all_scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  row_spec(lowest_rmse_index, bold = TRUE, background = "#DFF0D8") %>%
  kable_styling(latex_options = "striped")
```


>> Looking at the accuracy results for the four different time series models—ARIMA, TBATS, NNETAR, and StructTS— that were evaluated to forecast rye yields in the U.S. over the next 10 years. We can see that NNETAR stand out. Each model was assessed using standard accuracy metrics, including Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Scaled Error (MASE), among others.

>> The NNETAR model has the lowest RMSE (1.91) and MAE (1.55), indicating superior forecasting accuracy compared to the other models. It also posted the lowest MASE (0.82), reinforcing its relative strength in capturing patterns in the training data. Although its Mean Percentage Error (MPE) was slightly negative (-1.25), this implies a modest tendency to underpredict.

>> The ARIMA model performed reasonably well, with a slightly higher RMSE (1.99) and MAE (1.66), but it maintained a positive MPE (1.02) and low ACF1 (0.04), suggesting minimal autocorrelation in residuals—an indicator of good model fit.

> StructTS and TBATS both underperformed relative to NNETAR and ARIMA, with RMSE values exceeding 2.0 and higher MAE and MAPE scores. While TBATS had the highest ME and RMSE, its negative ACF1 value (-0.089) may point to some overfitting or residual autocorrelation that undercuts forecast reliability.




### Corn

#### Data exploration

#### Models

#### Accuracy and recommendations

#### Sources

- https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/regional/time-series
- https://www.fas.usda.gov/data/production/commodity/2222000
- https://www.ers.usda.gov/topics/crops/soybeans-and-oil-crops
- https://www.ers.usda.gov/data-products/commodity-costs-and-returns